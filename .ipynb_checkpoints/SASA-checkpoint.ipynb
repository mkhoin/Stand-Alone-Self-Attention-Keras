{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "import math\n",
    "\n",
    "class AttentionConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, groups=1, bias=False):\n",
    "        super(AttentionConv, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.groups = groups  # 여기서 groups의 수는 들어오는 인풋을 몇 개의 group으로 나눠서 컨볼루션 진행할 거냐를 결정합니다. 여기서 각 나뉘어진\n",
    "        # 그룹을 헤드(head)라고 부릅니다.\n",
    "\n",
    "        assert self.out_channels % self.groups == 0, \"out_channels should be divided by groups. (example: out_channels: 40, groups: 4)\"\n",
    "        # (당연히) 나가는 채널의 수가 처리하는 헤드 수로 나눌 수 있어야 합니다. 만약 그렇지 않으면 에러를 냅니다.\n",
    "\n",
    "        self.rel_h = nn.Parameter(torch.randn(out_channels // 2, 1, 1, kernel_size, 1), requires_grad=True)\n",
    "        self.rel_w = nn.Parameter(torch.randn(out_channels // 2, 1, 1, 1, kernel_size), requires_grad=True)\n",
    "        # 여기서 위 두 변수는 중심이 되는 픽셀에 대하여 상대 위치(height와 width)를 정하는 파라미터인데요, 역시 훈련 가능한 가중치입니다.\n",
    "        # 아시다시피 우리가 컨볼루션을 할 때 kernel을 왼쪽 위에서부터 오른쪽 아래로 움직이면서 계산을 하는데요, 중심이 되는 픽셀이란 그 kernel의 중심이\n",
    "        # 되는 부분에 해당하는 픽셀을 의미합니다. 때문에 그 중심 픽셀에 대해 상대 위치를 표현하기 위해서는 kernel size 만큼의 parameter가\n",
    "        # 필요한 것이죠. 예를 들면, kernel size가 3x3인 경우, 다음과 같이 상대 위치를 표현할 수 있습니다. 중심이 되는 픽셀의 위치가 (0, 0) 입니다.\n",
    "        # (-1, -1) (0, -1) (1, -1)\n",
    "        # (-1, 0)  (0, 0)  (1, 0)\n",
    "        # (-1, 1)  (0, 1)  (1, 1)\n",
    "\n",
    "        self.key_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n",
    "        self.query_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n",
    "        self.value_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n",
    "        # 인풋을 key, query, 그리고 value 각각에 대해 1x1 convolution으로 연산합니다.\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, channels, height, width = x.size()\n",
    "\n",
    "        padded_x = F.pad(x, [self.padding, self.padding, self.padding, self.padding])  # 왼쪽 오른쪽 위 아래에 대해 패딩해줍니다.\n",
    "        q_out = self.query_conv(x)\n",
    "        k_out = self.key_conv(padded_x)\n",
    "        v_out = self.value_conv(padded_x)\n",
    "\n",
    "        k_out = k_out.unfold(2, self.kernel_size, self.stride).unfold(3, self.kernel_size, self.stride)\n",
    "        v_out = v_out.unfold(2, self.kernel_size, self.stride).unfold(3, self.kernel_size, self.stride)\n",
    "        # unfold는 인풋을 명시된 차원에 대해 잘라주는 역할을 합니다. 예를 들면, 다음과 같습니다.\n",
    "        # x = torch.arange(1., 8)\n",
    "        # tensor([1., 2., 3., 4., 5., 6., 7.])\n",
    "        # x.unfold(0, 2, 1)\n",
    "        # tensor([[1., 2.],\n",
    "        #         [2., 3.],\n",
    "        #         [3., 4.],\n",
    "        #         [4., 5.],\n",
    "        #         [5., 6.],\n",
    "        #         [6., 7.]])\n",
    "        # x.unfold(0, 2, 2)\n",
    "        # tensor([[1., 2.],\n",
    "        #         [3., 4.],\n",
    "        #         [5., 6.]])\n",
    "        # 파이토치의 경우 텐서의 형태가 배치x채널x세로x가로이기 때문에 2차원과 3차원에 대해서 unfold한다는 건 이미지의 가로 세로를 잘라서 저장하겠다는\n",
    "        # 겁니다. 이 부분이 약간 헷갈리실 수 있는데요, 가로 세로가 5x5인 인풋을 3x3으로 자른다고 한다면 보폭이 1일 때 왼쪽 위부터 오른쪽 아래까지 총\n",
    "        # 9개의 3x3 이미지가 나오겠죠? 그걸 분리해서 저장하겠다는 의미입니다.\n",
    "\n",
    "        v_out_h, v_out_w = v_out.split(self.out_channels // 2, dim=1)  # value out 채널을 둘로 나누는 작업입니다.\n",
    "        v_out = torch.cat((v_out_h + self.rel_h, v_out_w + self.rel_w), dim=1)  # 그리고 나눠진 두 채널에 하나는 가로에 대한\n",
    "        # 상대위치 가중치를 더하고, 나머지에는 세로에 대한 상대위치 가중치를 더하여 다시 합칩니다. 이 부분을 나눠서 하는 이유는 좀 더 고민해 봐야 할 것\n",
    "        # 같습니다. ^^;\n",
    "\n",
    "        k_out = k_out.contiguous().view(batch, self.groups, self.out_channels // self.groups, height, width, -1)\n",
    "        v_out = v_out.contiguous().view(batch, self.groups, self.out_channels // self.groups, height, width, -1)\n",
    "        # key와 value의 텐서 형태를 바꾸는 작업(view)입니다. 여기서 주의할 건 텐서를 이루고 있는 요소 수는 변하지 않습니다. 예를 들어 2x3의 경우\n",
    "        # 1x6 또는 3x2와 같이 변경할 수 있지만 2x4처럼 요소 수가 변할 수는 없다는 것이죠.\n",
    "\n",
    "        q_out = q_out.view(batch, self.groups, self.out_channels // self.groups, height, width, 1)\n",
    "\n",
    "        out = q_out * k_out\n",
    "        out = F.softmax(out, dim=-1) # q와 k를 곱한 값에 softmax를 해주는 이유는 중심이 되는 픽셀인 q와 그 이웃한 픽셀들이 얼마나 관련이\n",
    "        # 있는지를 확률적으로 나타내기 위해서입니다. 위에서 q는 1x1 convolution만 해주고, k의 경우에는 kernel size만큼 이미지를 잘라서 저장한\n",
    "        # 사실을 기억해주시기 바랍니다. 즉, q는 인풋의 각 픽셀에 대한 정보를, k는 각 픽셀을 포함한 주변 정보를 담고 있는 셈이죠. :)\n",
    "        out = torch.einsum('bnchwk,bnchwk -> bnchw', out, v_out).view(batch, -1, height, width)\n",
    "        # einsum은 Einstein summation의 약자인데요, 텐서의 차원이 여러 개일 때 어떤 차원에 대해서 행렬곱을 해줄지 지정해주는 것에 불과합니다.\n",
    "        # 여기서는 'bnchwk'라는 6개의 차원 중에 k라는 차원에 대해 행렬곱을 해달라고 하는 것이죠. view는 위에 나왔던 것과 마찬가지로 텐서의 형태를\n",
    "        # 변경하겠다 하는 것입니다.\n",
    "\n",
    "        return out\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # 아래는 parameter의 초기값을 정해주는 역할을 합니다.\n",
    "        init.kaiming_normal_(self.key_conv.weight, mode='fan_out', nonlinearity='relu')\n",
    "        init.kaiming_normal_(self.value_conv.weight, mode='fan_out', nonlinearity='relu')\n",
    "        init.kaiming_normal_(self.query_conv.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "        init.normal_(self.rel_h, 0, 1)\n",
    "        init.normal_(self.rel_w, 0, 1)\n",
    "\n",
    "# AttentionStem은 전체 network의 가장 처음 연산하는 곳에 적용되는 부분입니다. 이 부분을 AttentionConv와 별도로 만들어준 이유는 classification\n",
    "# model의 처음 부분이 하는 역할과 나중 부분이 하는 역할이 다르기 때문입니다. Classification 모델의 처음 부분은 edge detector의 역할을 하는 반면\n",
    "# 나중 부분은 abstract한 feature를 추출하죠.\n",
    "\n",
    "class AttentionStem(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, groups=1, m=4, bias=False):\n",
    "        super(AttentionStem, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.groups = groups\n",
    "        self.m = m  # m(multiple)개 만큼 value matrix를 만듭니다.\n",
    "\n",
    "        assert self.out_channels % self.groups == 0, \"out_channels should be divided by groups. (example: out_channels: 40, groups: 4)\"\n",
    "\n",
    "        self.emb_a = nn.Parameter(torch.randn(out_channels // groups, kernel_size), requires_grad=True)\n",
    "        self.emb_b = nn.Parameter(torch.randn(out_channels // groups, kernel_size), requires_grad=True)\n",
    "        self.emb_mix = nn.Parameter(torch.randn(m, out_channels // groups), requires_grad=True)\n",
    "        # emb_a, emb_b, emb_mix는 AttentionConv와 비슷하게 상대위치를 결정해주는 역할을 합니다.\n",
    "\n",
    "        self.key_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n",
    "        self.query_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n",
    "        self.value_conv = nn.ModuleList([nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias) for _ in range(m)])\n",
    "        # value matrix를 m개 만큼 만드는 convolution을 정의합니다.\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, channels, height, width = x.size()\n",
    "\n",
    "        padded_x = F.pad(x, [self.padding, self.padding, self.padding, self.padding])\n",
    "\n",
    "        q_out = self.query_conv(x)\n",
    "        k_out = self.key_conv(padded_x)\n",
    "        v_out = torch.stack([self.value_conv[_](padded_x) for _ in range(self.m)], dim=0)\n",
    "\n",
    "        k_out = k_out.unfold(2, self.kernel_size, self.stride).unfold(3, self.kernel_size, self.stride)\n",
    "        v_out = v_out.unfold(3, self.kernel_size, self.stride).unfold(4, self.kernel_size, self.stride)\n",
    "        # 이 작업은 AttentionConv에서 했던 것과 동일하죠. 가로 세로를 kernel 사이즈 만큼 개별로 잘라서 보관하는 작업입니다.\n",
    "\n",
    "        k_out = k_out[:, :, :height, :width, :, :]\n",
    "        v_out = v_out[:, :, :, :height, :width, :, :]  # k_out보다 차원이 하나 더 많은 것은 value matrix를 m개 만큼 생성했기\n",
    "        # 때문입니다. 여기서 0번째 차원이 m값을 가집니다.\n",
    "\n",
    "        emb_logit_a = torch.einsum('mc,ca->ma', self.emb_mix, self.emb_a)\n",
    "        emb_logit_b = torch.einsum('mc,cb->mb', self.emb_mix, self.emb_b)\n",
    "        emb = emb_logit_a.unsqueeze(2) + emb_logit_b.unsqueeze(1)\n",
    "        # unsqueeze는 인자로 받은 차원을 만들어주는 역할을 합니다. 즉 emb_logit_a의 경우에는 m x a x 1,\n",
    "        # emb_logit_b의 경우에는 m x 1 x b의 형태를 갖게 됩니다. 여기서 a, b는 k(kernel size)와 동일합니다.\n",
    "\n",
    "        emb = F.softmax(emb.view(self.m, -1), dim=0).view(self.m, 1, 1, 1, 1, self.kernel_size, self.kernel_size)\n",
    "        # emb의 형태를 m x 1 x 1 x 1 x 1 x k x k 의 형태로 바꾼 후, softmax를 취해줍니다. 형태를 저렇게 바꿔주는 이유는 v_out과 행렬곱을\n",
    "        # 하게 하기 위함입니다.\n",
    "\n",
    "        v_out = emb * v_out\n",
    "\n",
    "        k_out = k_out.contiguous().view(batch, self.groups, self.out_channels // self.groups, height, width, -1)\n",
    "        v_out = v_out.contiguous().view(self.m, batch, self.groups, self.out_channels // self.groups, height, width, -1)\n",
    "        v_out = torch.sum(v_out, dim=0).view(batch, self.groups, self.out_channels // self.groups, height, width, -1)\n",
    "        # v_out의 0번째 차원은 처음에 만든 m개의 value matrix를 포함하고 있는데요, 0번째 차원을 더함으로써 그 차원을 없앱니다.\n",
    "\n",
    "        # 아래 부분은 AttentionConv에서 하는 연산과 동일합니다.\n",
    "        q_out = q_out.view(batch, self.groups, self.out_channels // self.groups, height, width, 1)\n",
    "\n",
    "        out = q_out * k_out\n",
    "        out = F.softmax(out, dim=-1)\n",
    "        out = torch.einsum('bnchwk,bnchwk->bnchw', out, v_out).view(batch, -1, height, width)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # 가중치를 초기화 하는 부분입니다.\n",
    "        init.kaiming_normal_(self.key_conv.weight, mode='fan_out', nonlinearity='relu')\n",
    "        init.kaiming_normal_(self.query_conv.weight, mode='fan_out', nonlinearity='relu')\n",
    "        for _ in self.value_conv:\n",
    "            init.kaiming_normal_(_.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "        init.normal_(self.emb_a, 0, 1)\n",
    "        init.normal_(self.emb_b, 0, 1)\n",
    "        init.normal_(self.emb_mix, 0, 1)\n",
    "\n",
    "\n",
    "# temp = torch.randn((2, 3, 32, 32))\n",
    "# conv = AttentionConv(3, 16, kernel_size=3, padding=1)\n",
    "# print(conv(temp).size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
