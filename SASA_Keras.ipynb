{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tykimos/Projects/venv3/lib/python3.5/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 12)   48          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_augmentation2d_1 (Att (None, 32, 32, 4)    126         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 4)    20          attention_augmentation2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 20)   0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 642\n",
      "Trainable params: 642\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Attention Augmented Conv out shape :  (1, 32, 32, 20)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Layer\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import concatenate\n",
    "\n",
    "from keras import initializers\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def _conv_layer(filters, kernel_size, strides=(1, 1), padding='same', name=None):\n",
    "    return Conv2D(filters, kernel_size, strides=strides, padding=padding,\n",
    "                  use_bias=True, kernel_initializer='he_normal', name=name)\n",
    "\n",
    "\n",
    "def _normalize_depth_vars(depth_k, depth_v, filters):\n",
    "    \"\"\"\n",
    "    Accepts depth_k and depth_v as either floats or integers\n",
    "    and normalizes them to integers.\n",
    "    Args:\n",
    "        depth_k: float or int.\n",
    "        depth_v: float or int.\n",
    "        filters: number of output filters.\n",
    "    Returns:\n",
    "        depth_k, depth_v as integers.\n",
    "    \"\"\"\n",
    "\n",
    "    if type(depth_k) == float:\n",
    "        depth_k = int(filters * depth_k)\n",
    "    else:\n",
    "        depth_k = int(depth_k)\n",
    "\n",
    "    if type(depth_v) == float:\n",
    "        depth_v = int(filters * depth_v)\n",
    "    else:\n",
    "        depth_v = int(depth_v)\n",
    "\n",
    "    return depth_k, depth_v\n",
    "\n",
    "\n",
    "class AttentionAugmentation2D(Layer):\n",
    "\n",
    "    def __init__(self, depth_k, depth_v, num_heads, relative=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Applies attention augmentation on a convolutional layer\n",
    "        output.\n",
    "        Args:\n",
    "            depth_k: float or int. Number of filters for k.\n",
    "            Computes the number of filters for `v`.\n",
    "            If passed as float, computed as `filters * depth_k`.\n",
    "        depth_v: float or int. Number of filters for v.\n",
    "            Computes the number of filters for `k`.\n",
    "            If passed as float, computed as `filters * depth_v`.\n",
    "        num_heads: int. Number of attention heads.\n",
    "            Must be set such that `depth_k // num_heads` is > 0.\n",
    "        relative: bool, whether to use relative encodings.\n",
    "        Raises:\n",
    "            ValueError: if depth_v or depth_k is not divisible by\n",
    "                num_heads.\n",
    "        Returns:\n",
    "            Output tensor of shape\n",
    "            -   [Batch, Height, Width, Depth_V] if\n",
    "                channels_last data format.\n",
    "            -   [Batch, Depth_V, Height, Width] if\n",
    "                channels_first data format.\n",
    "        \"\"\"\n",
    "        super(AttentionAugmentation2D, self).__init__(**kwargs)\n",
    "\n",
    "        if depth_k % num_heads != 0:\n",
    "            raise ValueError('`depth_k` (%d) is not divisible by `num_heads` (%d)' % (\n",
    "                depth_k, num_heads))\n",
    "\n",
    "        if depth_v % num_heads != 0:\n",
    "            raise ValueError('`depth_v` (%d) is not divisible by `num_heads` (%d)' % (\n",
    "                depth_v, num_heads))\n",
    "\n",
    "        if depth_k // num_heads < 1.:\n",
    "            raise ValueError('depth_k / num_heads cannot be less than 1 ! '\n",
    "                             'Given depth_k = %d, num_heads = %d' % (\n",
    "                             depth_k, num_heads))\n",
    "\n",
    "        if depth_v // num_heads < 1.:\n",
    "            raise ValueError('depth_v / num_heads cannot be less than 1 ! '\n",
    "                             'Given depth_v = %d, num_heads = %d' % (\n",
    "                                 depth_v, num_heads))\n",
    "\n",
    "        self.depth_k = depth_k\n",
    "        self.depth_v = depth_v\n",
    "        self.num_heads = num_heads\n",
    "        self.relative = relative\n",
    "\n",
    "        self.axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self._shape = input_shape\n",
    "\n",
    "        # normalize the format of depth_v and depth_k\n",
    "        self.depth_k, self.depth_v = _normalize_depth_vars(self.depth_k, self.depth_v,\n",
    "                                                           input_shape)\n",
    "\n",
    "        if self.axis == 1:\n",
    "            _, channels, height, width = input_shape\n",
    "        else:\n",
    "            _, height, width, channels = input_shape\n",
    "\n",
    "        if self.relative:\n",
    "            dk_per_head = self.depth_k // self.num_heads\n",
    "\n",
    "            if dk_per_head == 0:\n",
    "                print('dk per head', dk_per_head)\n",
    "\n",
    "            self.key_relative_w = self.add_weight('key_rel_w',\n",
    "                                                  shape=[2 * width - 1, dk_per_head],\n",
    "                                                  initializer=initializers.RandomNormal(\n",
    "                                                      stddev=dk_per_head ** -0.5))\n",
    "\n",
    "            self.key_relative_h = self.add_weight('key_rel_h',\n",
    "                                                  shape=[2 * height - 1, dk_per_head],\n",
    "                                                  initializer=initializers.RandomNormal(\n",
    "                                                      stddev=dk_per_head ** -0.5))\n",
    "\n",
    "        else:\n",
    "            self.key_relative_w = None\n",
    "            self.key_relative_h = None\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if self.axis == 1:\n",
    "            # If channels first, force it to be channels last for these ops\n",
    "            inputs = K.permute_dimensions(inputs, [0, 2, 3, 1])\n",
    "\n",
    "        q, k, v = tf.split(inputs, [self.depth_k, self.depth_k, self.depth_v], axis=-1)\n",
    "\n",
    "        q = self.split_heads_2d(q)\n",
    "        k = self.split_heads_2d(k)\n",
    "        v = self.split_heads_2d(v)\n",
    "\n",
    "        # scale query\n",
    "        depth_k_heads = self.depth_k / self.num_heads\n",
    "        q *= (depth_k_heads ** -0.5)\n",
    "\n",
    "        # [Batch, num_heads, height * width, depth_k or depth_v] if axis == -1\n",
    "        qk_shape = [self._batch, self.num_heads, self._height * self._width, self.depth_k // self.num_heads]\n",
    "        v_shape = [self._batch, self.num_heads, self._height * self._width, self.depth_v // self.num_heads]\n",
    "        flat_q = K.reshape(q, K.stack(qk_shape))\n",
    "        flat_k = K.reshape(k, K.stack(qk_shape))\n",
    "        flat_v = K.reshape(v, K.stack(v_shape))\n",
    "\n",
    "        # [Batch, num_heads, HW, HW]\n",
    "        logits = tf.matmul(flat_q, flat_k, transpose_b=True)\n",
    "\n",
    "        # Apply relative encodings\n",
    "        if self.relative:\n",
    "            h_rel_logits, w_rel_logits = self.relative_logits(q)\n",
    "            logits += h_rel_logits\n",
    "            logits += w_rel_logits\n",
    "\n",
    "        weights = K.softmax(logits, axis=-1)\n",
    "        attn_out = tf.matmul(weights, flat_v)\n",
    "\n",
    "        attn_out_shape = [self._batch, self.num_heads, self._height, self._width, self.depth_v // self.num_heads]\n",
    "        attn_out_shape = K.stack(attn_out_shape)\n",
    "        attn_out = K.reshape(attn_out, attn_out_shape)\n",
    "        attn_out = self.combine_heads_2d(attn_out)\n",
    "        # [batch, height, width, depth_v]\n",
    "\n",
    "        if self.axis == 1:\n",
    "            # return to [batch, depth_v, height, width] for channels first\n",
    "            attn_out = K.permute_dimensions(attn_out, [0, 3, 1, 2])\n",
    "\n",
    "        return attn_out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_shape = list(input_shape)\n",
    "        output_shape[self.axis] = self.depth_v\n",
    "        return tuple(output_shape)\n",
    "\n",
    "    def split_heads_2d(self, ip):\n",
    "        tensor_shape = K.shape(ip)\n",
    "\n",
    "        # batch, height, width, channels for axis = -1\n",
    "        tensor_shape = [tensor_shape[i] for i in range(len(self._shape))]\n",
    "\n",
    "        batch = tensor_shape[0]\n",
    "        height = tensor_shape[1]\n",
    "        width = tensor_shape[2]\n",
    "        channels = tensor_shape[3]\n",
    "\n",
    "        # Save the spatial tensor dimensions\n",
    "        self._batch = batch\n",
    "        self._height = height\n",
    "        self._width = width\n",
    "\n",
    "        ret_shape = K.stack([batch, height, width,  self.num_heads, channels // self.num_heads])\n",
    "        split = K.reshape(ip, ret_shape)\n",
    "        transpose_axes = (0, 3, 1, 2, 4)\n",
    "        split = K.permute_dimensions(split, transpose_axes)\n",
    "\n",
    "        return split\n",
    "\n",
    "    def relative_logits(self, q):\n",
    "        shape = K.shape(q)\n",
    "        # [batch, num_heads, H, W, depth_v]\n",
    "        shape = [shape[i] for i in range(5)]\n",
    "\n",
    "        height = shape[2]\n",
    "        width = shape[3]\n",
    "\n",
    "        rel_logits_w = self.relative_logits_1d(q, self.key_relative_w, height, width,\n",
    "                                               transpose_mask=[0, 1, 2, 4, 3, 5])\n",
    "\n",
    "        rel_logits_h = self.relative_logits_1d(\n",
    "            K.permute_dimensions(q, [0, 1, 3, 2, 4]),\n",
    "            self.key_relative_h, width, height,\n",
    "            transpose_mask=[0, 1, 4, 2, 5, 3])\n",
    "\n",
    "        return rel_logits_h, rel_logits_w\n",
    "\n",
    "    def relative_logits_1d(self, q, rel_k, H, W, transpose_mask):\n",
    "        rel_logits = tf.einsum('bhxyd,md->bhxym', q, rel_k)\n",
    "        rel_logits = K.reshape(rel_logits, [-1, self.num_heads * H, W, 2 * W - 1])\n",
    "        rel_logits = self.rel_to_abs(rel_logits)\n",
    "        rel_logits = K.reshape(rel_logits, [-1, self.num_heads, H, W, W])\n",
    "        rel_logits = K.expand_dims(rel_logits, axis=3)\n",
    "        rel_logits = K.tile(rel_logits, [1, 1, 1, H, 1, 1])\n",
    "        rel_logits = K.permute_dimensions(rel_logits, transpose_mask)\n",
    "        rel_logits = K.reshape(rel_logits, [-1, self.num_heads, H * W, H * W])\n",
    "        return rel_logits\n",
    "\n",
    "    def rel_to_abs(self, x):\n",
    "        shape = K.shape(x)\n",
    "        shape = [shape[i] for i in range(3)]\n",
    "        B, Nh, L, = shape\n",
    "        col_pad = K.zeros(K.stack([B, Nh, L, 1]))\n",
    "        x = K.concatenate([x, col_pad], axis=3)\n",
    "        flat_x = K.reshape(x, [B, Nh, L * 2 * L])\n",
    "        flat_pad = K.zeros(K.stack([B, Nh, L - 1]))\n",
    "        flat_x_padded = K.concatenate([flat_x, flat_pad], axis=2)\n",
    "        final_x = K.reshape(flat_x_padded, [B, Nh, L + 1, 2 * L - 1])\n",
    "        final_x = final_x[:, :, :L, L - 1:]\n",
    "        return final_x\n",
    "\n",
    "    def combine_heads_2d(self, inputs):\n",
    "        # [batch, num_heads, height, width, depth_v // num_heads]\n",
    "        transposed = K.permute_dimensions(inputs, [0, 2, 3, 1, 4])\n",
    "        # [batch, height, width, num_heads, depth_v // num_heads]\n",
    "        shape = K.shape(transposed)\n",
    "        shape = [shape[i] for i in range(5)]\n",
    "\n",
    "        a, b = shape[-2:]\n",
    "        ret_shape = K.stack(shape[:-2] + [a * b])\n",
    "        # [batch, height, width, depth_v]\n",
    "        return K.reshape(transposed, ret_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'depth_k': self.depth_k,\n",
    "            'depth_v': self.depth_v,\n",
    "            'num_heads': self.num_heads,\n",
    "            'relative': self.relative,\n",
    "        }\n",
    "        base_config = super(AttentionAugmentation2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "def augmented_conv2d(ip, filters, kernel_size=(3, 3), strides=(1, 1),\n",
    "                     depth_k=0.2, depth_v=0.2, num_heads=8, relative_encodings=True):\n",
    "    \"\"\"\n",
    "    Builds an Attention Augmented Convolution block.\n",
    "    Args:\n",
    "        ip: keras tensor.\n",
    "        filters: number of output filters.\n",
    "        kernel_size: convolution kernel size.\n",
    "        strides: strides of the convolution.\n",
    "        depth_k: float or int. Number of filters for k.\n",
    "            Computes the number of filters for `v`.\n",
    "            If passed as float, computed as `filters * depth_k`.\n",
    "        depth_v: float or int. Number of filters for v.\n",
    "            Computes the number of filters for `k`.\n",
    "            If passed as float, computed as `filters * depth_v`.\n",
    "        num_heads: int. Number of attention heads.\n",
    "            Must be set such that `depth_k // num_heads` is > 0.\n",
    "        relative_encodings: bool. Whether to use relative\n",
    "            encodings or not.\n",
    "    Returns:\n",
    "        a keras tensor.\n",
    "    \"\"\"\n",
    "    # input_shape = K.int_shape(ip)\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "\n",
    "    depth_k, depth_v = _normalize_depth_vars(depth_k, depth_v, filters)\n",
    "\n",
    "    conv_out = _conv_layer(filters - depth_v, kernel_size, strides)(ip)\n",
    "\n",
    "    # Augmented Attention Block\n",
    "    qkv_conv = _conv_layer(2 * depth_k + depth_v, (1, 1), strides)(ip)\n",
    "    attn_out = AttentionAugmentation2D(depth_k, depth_v, num_heads, relative_encodings)(qkv_conv)\n",
    "    attn_out = _conv_layer(depth_v, kernel_size=(1, 1))(attn_out)\n",
    "\n",
    "    output = concatenate([conv_out, attn_out], axis=channel_axis)\n",
    "    return output\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from keras.layers import Input\n",
    "    from keras.models import Model\n",
    "\n",
    "    ip = Input(shape=(32, 32, 3))\n",
    "    x = augmented_conv2d(ip, filters=20, kernel_size=(3, 3),\n",
    "                         depth_k=0.2, depth_v=0.2,  # dk/v (0.2) * f_out (20) = 4\n",
    "                         num_heads=4, relative_encodings=True)\n",
    "\n",
    "    model = Model(ip, x)\n",
    "    model.summary()\n",
    "\n",
    "    # Check if attention builds properly\n",
    "    x = tf.zeros((1, 32, 32, 3))\n",
    "    y = model(x)\n",
    "    print(\"Attention Augmented Conv out shape : \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
